# A Brief History of Dat
by [Max Ogden](http://maxogden.com)

We have some exciting news to share about dat: We're closing in on a 1.0 release! It's not out just yet, but you can try what we have so far by checking out [the `master` branch on GitHub](https://github.com/maxogden/dat) or with `npm install dat@next -g`.

I'd like to explain the history of the project and the design choices over the two years since the project started, because the project has made some pretty substantial changes in direction during that time. We have been in a constant cycle of R&D during this period, and have rewritten the dat CLI four times.

Each rewrite incorporates the best modules and approaches from the last iteration, and presents what we hope is a more straightforward workflow. We firmly believe this iterative process is the only way we could have arrived at the conclusions that led us to the features that we’re focusing on for our 1.0 release.

## Dat Prototype

The original use case for dat centered around tabular datasets that change often. We wanted to make something to simplify the process of updating your copy of data when the publisher updates the source data. The prototype version of Dat was built over the course of 6 months, and was completed in the spring of last year.

The prototype `dat` command-line tool only worked with tabular data, and usage looked like this:

```sh
dat init
echo '{"hello": "world"}' | dat --json # put a JSON object into dat
cat some_csv.csv | dat --csv # stream a CSV into dat
echo $'a,b,c\n1,2,3' | dat --csv --primary=a # specify a primary key to use
dat cat # stream the most recent of all rows
dat push http://mydat.myserver.com:6461
```

In addition to `dat push` there was also `dat clone` and `dat pull`.

An example of the canonical use case we had in mind at the time was an Excel spreadsheet that gets published on an FTP server. We envisioned dat as a tool that could sit between the user and the Excel spreadsheet, adding fast, efficient syncing of new updates, as well as version control features—neither of these are supported by Excel natively.

## Dat Alpha

The alpha version of dat was [released in August of 2014](https://usopendata.org/2014/08/19/dat-alpha/). The major feature we worked on was support for syncing large, non-tabular data files. This opened up a new use case: using dat as a sort of “DropBox for data” to sync a folder on your filesystem.

The alpha release was the first after starting on a new grant that shifted our focus from open civic datasets (which tend to be tabular -- i.e. lots of database tables) to the field of data-intensive scientific research, which tends to use domain-specific flat-file-based data formats.

We’ve worked directly with some amazing research labs in the fields astrophysics, bioinformatics, and neuroscience to  understand their data management problems. The addition of large file support in the alpha was a direct result of getting feedback from these scientific pilot users.

With the alpha CLI we attempted to support both the tabular *and* file-syncing use cases, which increased the API surface area quite a bit:

```
dat cat
dat export
dat import
dat init
dat help
dat version
dat pull
dat push
dat clean
dat clone
dat serve
dat listen
dat blobs get
dat blobs put
dat rows get
dat rows delete
dat rows put
```

We were very excited about the new file- (AKA "blob") syncing use cases, but we were unsure about the intuitiveness of the API that we’d devised. User testing during this period revealed that when given a choice between two workflows, new users became confused. This made it harder to get started.

To puttabular data into version control, you first have to know your schema, know your primary key (or come up with a composite primary key or use random unique IDs), and then build an import process that can be repeated when your source tabular data files change. This was a lot of work for users before they got to the gratifying parts, like convenient push/pull/sync.

Also because we presented separate “row-oriented“ and “blob-oriented” workflows, users needed to understand the tradeoffs of both before making a choice, further complicating the onboarding process and the path to a first gratifying experience.

## Dat Beta

The beta version [shipped in July](https://usopendata.org/2015/07/29/dat-beta/). The major focus was to make collaboration and reproducibility features possible by switching our internal data representation to a [directed acyclic graph](https://github.com/jbenet/random-ideas/issues/20). This means that we can model fully decentralized workflows like pull requests on top of dat, as well as offer versioning with cryptographic accuracy for entire datasets.

Up until this version dat acted more like a traditional centralized version control system (CVS), like [Subversion](https://subversion.apache.org/), where there is a central repository and all clients must synchronize with the central database before they can send any change they made.

The new DAG abstraction we developed during this time is called [hyperlog](https://github.com/mafintosh/hyperlog). It provides a graph storage API that supports incremental, streaming replication. We used it in the `dat` CLI tool as the core database, and added tabular import and file import & synchronization features on top of it.

In terms of the command-line API, the beta didn’t change dramatically from the alpha. We still supported both tabular- and file-oriented workflows. We dropped the word “blob” in exchange for “files” and used terms like “read” and “write” instead of “get” and “put.”

The most notable new concept in the beta was support for multiple datasets in a single repository. This was added to support datasets with hybrid data types, such as an astronomy full-sky scan which might include raw image files from a telescope, as well as tabular data created during post-processing of the images. You could model each one as a dataset in dat (like two different sub-folders).

```
repository commands:
  dat init        Create a new dat.
  dat clone       Copy a dat to the local filesystem via http or ssh.
  dat push        Push data to a remote dat.
  dat pull        Pull data from a remote dat.
  dat checkout    Change view to a given version.
  dat serve       Start an http server.

descriptive commands:
  dat status      Show current status.
  dat log         List of changes.
  dat files       List all files.
  dat datasets    List all datasets.
  dat forks       List current forks.
  dat diff        See differences between the data in two forks.
  dat keys        List existing keys in a dataset.

data commands:
  dat import      Add tabular data to a dataset.
  dat export      View tabular data from a dataset.
  dat read        Read a binary file.
  dat write       Write a binary file.
  dat delete      Delete a key in a dataset.
  dat merge       Merge two forks into one.
```

At the time of the beta release we were most excited about the new decentralized possibilities. We were still unsure about the intuitiveness of the API, but weren’t sure how to simplify the API without dropping support for use cases we thought were important.

## Dat 1.0

The Dat 1.0 is a work in progress and is not out at the time of this post. TODO
