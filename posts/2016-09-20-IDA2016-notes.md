# International Data Week 2016 - Meeting Notes and Links

*[International Data Week](internationaldataweek.org) in September 2016 brought together three events, [SciDataCon](http://www.scidatacon.org/), the International Data Forum, and the [Research Data Alliance 8th plenary](https://rd-alliance.org/plenaries/rda-eighth-plenary-meeting-denver-co). Joe Hand from the Dat team attended the conference and collected his notes here. This is a brain dump from the conference, check out [Joe's reflection post](http://LINKME) from International Data Week too for a more organized summary.*

## SciDataCon2016

SciDataCon featured sessions with 5-10 quick lightning talk each on a common theme. Notes from some of the more interesting sessions are below.

### [Session on Sustainable Business Models for Data Repositories](http://www.scidatacon.org/2016/sessions/45/)

These two sessions covered the costs, budgets, and revenue streams for various data repositories. There are many different pricing structures. Details on these can be found in the witness statements in the session link above. 

Repositories are using a mix of subscriptions (paid by libraries, insitutions, researchers), pay to download (to get specific datasets), or pay to deposit (pay to upload to the repository), and external funding (government, private). 

While each pricing model has its own strengths and downsides, it was clear the loser is the academic institutions, libraries, and researchers. Several library managers and researchers expressed concern that soon they will be unable to keep up financially with the number of data repository subscriptions they need to get data, similar to the issues being faced with journal subscriptions. Additionally, as more data becomes replicated in several places, it becomes unclear which repositories should be subscribed to.

From the perspective of Dat, the costs were the most interesting part. For many of the data repositories hosting and bandwidth covers 1/4th to 1/3rd of the costs. We hope that Dat will be able to better distribute or reduce a lot of those costs.

Repositories that offer *active* dataset curation also had a high cost for the curators, a relatively new profession, at around 1/3 of the total costs.

Finally, the last big chunk of spending was on software development and general research and development. This is not surprising, software is expensive. But it was surprising to see how many repositories were replicating the same costs developing similar software.

### [Managing Science Data: A Federal Agency’s Perspective](http://www.scidatacon.org/2016/sessions/10/)

This session featured the challenges faced by federal agencies around data management. Recent federal mandates have started to require open data and agencies are moving towards that goal. The papers from the session are worth a read for the different perspective, compared to academic researchers.

The [USGS data management plan](https://www2.usgs.gov/datamanagement/why.php) is a well thought out plan on each stage of the data lifecycle, how the USGS is approaching it, and summaries of the federal requirements around data.

## International Data Forum

The highlight of the forum was the panel on "Open Data as a Public Good and the Responsibilities of Scientists".

The three presentation slides are available online:

* Myron P. Gutmann, University of Colorado Boulder: [“Balancing Scientific Responsibility and the Public Good”](https://docs.google.com/viewer?url=http%3A%2F%2Fwww.internationaldataweek.org%2Fsites%2Fdefault%2Ffiles%2FMyron%2520P.%2520Gutmann.pptx%23overlay-context%3DInternational-data-forum)
* Victoria Stodden, University of Illinois at Urbana-Champaign: [“Scientific Transparency, Integrity and Reproducibility”](http://www.internationaldataweek.org/sites/default/files/Victoria%20Stodden.pdf#overlay-context=International-data-forum)
* Takashi Onishi, President of Science Council of Japan (SCJ): [“Open Data as Game Changer for Natural Disaster Prevention”](https://docs.google.com/viewer?url=http%3A%2F%2Fwww.internationaldataweek.org%2Fsites%2Fdefault%2Ffiles%2FTakashi%2520Onishi.pptx%23overlay-context%3DInternational-data-forum)

The presenters made clear that is a strong case to made for why open data is good for the public, especially in cases like natural disaster prevention. However, it is unreasonable to say all research data must be totally open. There are privacy concerns, IP issues, and technological hurdles. Making specific recommendations, such as around reproducibility of published results, can go a long way in supporting both the public good and research needs.

## Research Data Alliance (RDA) 8th Plenary

The RDA plenary had sessions for working groups, interest groups, and birds of a feather groups (informal, single-time events). Each of these sessions involved some presentation with plenty of time for discussion.

Research Data Alliance is a collection of academics, librarians, and other groups involved with research data. RDA has [good primer](https://rd-alliance.org/rda-newcomers-some-notes) on their organization structure. They meet regularly with a focus on interest groups and working groups. Working groups and interest groups continue to communicate throughout the year to form recommendations (working groups) or help inform discussion throughout RDA (interest groups).

### Session Notes

Notes from specific sessions collected below.

#### [Long Tail Data IG Meeting](https://rd-alliance.org/ig-long-tail-research-data-rda-8th-plenary-meeting)

* The [interest group of long tail data](https://rd-alliance.org/groups/long-tail-research-data-ig.html) is very relevant to Dat. They are [drafting these recommendations](https://docs.google.com/document/d/1TL2VbuIs8mHDT4MZyK3jqkdSP7cL9O7GbMbapu_9CNQ/edit).
* Amazon succeeded by providing access to the long tail of books. There is a huge gap is services/infrastructure for the long tail of data. This long tail is from individual researchers. Generally small data (>1GB). **90% of researchers keep data on local, lab, or institution servers/backups, not in data repositories.** 
* Most data isn't highly structured or doesn't have standardized metadata. But this is where a lot of the innovation comes from. Newer research fields, newer types of data, etc. These data will be impossible to put in many data repositories.
* "Important to show immediate value upon deposit [to a data repository]". 
* "[Data] needs to link to other resources". 
* "Data that doesn't lead to publication is the data that currently gets lost".
* [Big data, little data, no data](https://mitpress.mit.edu/big-data-little-data-no-data) - Book that may be of interest.
* Orphan data are the bigger problem, data that don't have a home.
* Libraries will provide for a key partner in managing and publishing this type of data.
* Metadata is important but need to strike balance between ease of use and standardization/discovery.

#### [Joint meeting of IG Preservation e-Infrastructure, IG Reproducibility: Knowledge Preservation Tools for Researchers](https://rd-alliance.org/rda-8th-plenary-joint-meeting-ig-preservation-e-infrastructure%C2%A0ig-reproducibility)

* How can we foster relationships in overlap between researcher and archivist? Bridge the gap between researcher with data and preservation needs. What tools are necessary to make this easier?
* **If a researcher wants to preserve data, they have to share it.** Non-trivial thing and can be daunting for researchers.
* If a researcher can make your workflows work on remote computers, they can preserve it for reproducible execution.
* **"No generic preservation tool exists"** (speaker assertion). Must be some level of commonality to preserve all science & research inputs (data, software, workflow, etc.). Capture should complete for reproducibility. 
* "Not tool will be adopted unless there is economic incentive (enlightened self-interest)"
* What is meant by reproducibility? Trying to get the same answer or independently implement the same methodology to see what answer if given. There are always people reproducing others' work, usually students, postdocs. No way to publish or credit them. We know they are happening But not what happens or who is doing them. We only hear about them when they differ significantly from the original result. 
* **Major risk using these tools (data repository, online reproducibility software, etc.) is what happens if a tool get deprecated, loses funding, etc.**

#### [Data Versioning BOF](https://rd-alliance.org/data-versioning-rda-8th-plenary-bof-meeting)

* [Identification of Reproducible Subsets for Data Citation,
Sharing and Re-Use Guidelines](https://www.rd-alliance.org/system/files/documents/TCDL-RDA-Guidelines_160411.pdf)
* How to do data versioning with colocation with petascale computers and cloud intensive computing?
* Not just an issue for big data, also small datasets.
* query subsetting data is important; spatial queries useful
* "it is hard for a researcher to cite the exact data extract used to support investigation". particularly with subsets or dynamic queries
* **Battle over who assigns the DOI/PID for dataset and subsets of data**. If data is of value, institution or repository owner is prepared to store it. But who mints the DOI?
* Lots of different perspectives on how to version and what is relevant. 
* What happens when storage is full and old versions need to be discarded? How to achieve proper persistence/archiving while managing space?

### RDA Working Groups and Interest Groups of Interest

There are lots of interesting working groups and interest groups in the Research Data Alliance. A few already have specific recommendations out, that may overlap with Dat:

* [Data citation working group recommendation](https://rd-alliance.org/group/data-citation-wg/outcomes/data-citation-recommendation.html)
* [Persistent Identifiers recommendation](https://rd-alliance.org/system/files/PIT%20final%20report.pdf)
* (more soon)

There are also many RDA working groups and interest groups that are in progress, i.e. formalizing their recommendations, that overlap with Dat:

* Research data provenance
* Repository platforms for research data
* Publishing data working group
* E-infrastructure
* Persistent Identifiers (PID) interest group
* National data services
* **Long tail is research data** (very relevant)
* Data description interoperability
* Research data repository interoperability
* Publishing data services

---

## Links of Interest Collected Throughout IDW

* [DataFAIRport](http://www.datafairport.org/) - Their vision is to join and support existing communities that try to realize and enable a situation where valuable scientific data is ‘FAIR’ in the sense of being Findable, Accessible, Interoperable and Reusable.
* [Scholix](http://www.scholix.org/) (elsevier) - high level interoperability framework for exchanging information about the links between scholarly literature and data. It aims to build an open information ecosystem to understand systematically what data underpins literature and what literature references data.
* **[Maslow hierarchy for research data](https://www.elsevier.com/connect/10-aspects-of-highly-effective-research-data)** - This is really interesting to think about. Seems like Dat is focused more on the bottom two sections.
* [Preserving data at risk](http://www.codata.org/task-groups/data-at-risk)

### Data Repositories of Interest

* There are [over 1,500 data repositories](http://www.re3data.org/) according to the re3data registry.
* [BioCaddie](https://biocaddie.org/) - A *meta* data repository ( collection of other data repositories). Recently released, they've done a ton of work on ingesting other data repositories. 
* [ProteinDataBank](http://www.rcsb.org/pdb/home/home.do) - 555GB of protein data. Specifically, there may be good opportunity for Dat to test large scale sharing and versioning. They currently have [this versioning scheme](http://www.rcsb.org/pdb/static.do?p=download/ftp/index.html) and [downloads over FTP](http://www.wwpdb.org/download/downloads).
* [datasearch - elsevier](https://datasearch.elsevier.com/) -  *meta* data repository 
* [share - open science framework](https://share.osf.io/) -  *meta* data repository 

## Metadata, Standards, Interoperability

* Tons of discussion across many groups of metadata, standards, and interoperability. This is a huge problem. There are many many (way more than I though) specific domain or more generalized data repositories. Some of these have common metadata (usually in domain specific case) but lots of data repositories have their own and rely on translations to/from other metadata formats in order to improve interoperability.
* Metadata is great in that it allows you to do things like improved visualization, validation, discovery. 
* Data curation (making sure a repository has only high-quality data) relies very heavily on metadata.
* Many groups trying to figure out the perfect metadata so they can improve data discovery and curation. Some are supposed to be more general repositories and some domain specific.
