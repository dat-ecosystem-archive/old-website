# Reflections from International Data Week 2016

*[International Data Week](internationaldataweek.org) in September 2016 brought together three events, [SciDataCon](http://www.scidatacon.org/), the International Data Forum, and the [Research Data Alliance 8th plenary](https://rd-alliance.org/plenaries/rda-eighth-plenary-meeting-denver-co). Joe Hand from the Dat team attended the conference. Check out [Joe's  notes](http://LINKME) from International Data Week too.*

(TODO: intro on the three themes)

THEMES:
1. top-down publishing & data citation
2. reproducibility
3. the long tail (the little discussed theme)

## Metadata Standards and Data Publishing

The treatment of publishing and citation of academic papers is informing the discussion around data publishing. Researchers livelihoods rely on proper citation for their work. Citations allow them to get more grants and eventually secure tenure. Many groups in academia are approaching data publishing with similar goals of improving citability. There was a substantial focus on standardizing metadata and moving towards a common data citation process.

All data publishing systems showcased throughout the week had similar end goals:

* A researcher should be able to publish their data.
* Another research should be able to find, download, and use the data
* Any published work should cite the data used, crediting the researcher that originally published the data.

As data publishing exists today, there are no standard ways to cite data. Unlike papers, data may be modified and changed before being used in other research. Citing data will require the ability to cite versions of data, subsets of data, or forks of data in order to be reproducible. Many organizations are trying to solve the citation problem in parallel leading to the well-known standards problem:

![xkcd standards](http://imgs.xkcd.com/comics/standards.png)

## Reproducibility and Long Term Preservation 

Parallel to the discussion of data citation, reproducibility was a major theme of the week. When a researcher publishes a paper, other researchers should be able to independently verify the results. For many papers involving data or software reproducing results impossible today. Researchers do not always publish related data or software, and if they do it is not published in a manner that other researchers can re-run the analysis.

With the explosion in new data repositories we may be making headway as more researchers publish their data. The [re3data registry](http://re3data.org) has over 1500 data repositories registered. As discussed above, repositories have their own metadata formats, publishing processes, and different underlying technologies. Additionally, each repository has its own revenue, sustainability, and preservation methodologies.

Dissimilarity in data publishing has major implications for long term reproducibility. In the fast-moving era of the web, we see online services start up and shut down daily. Websites that were there one day are gone the next. Data powering these sites can disappear entirely as the users lose the countless hours they've contributed to the site. If data repositories are built with the same approach and technologies as existing web services we may face similar disappearances of mass troves of science data, with potentially disastrous results for reproducibility. Rising research costs and aging web infrastructure point to a stark future for published data.

### The Cascading Costs of Data Publishing

The current progress in data publishing could be lost as institutions and libraries continue to face rising costs around access to research. Libraries are already pushing back against rising journal prices. During the week, several librarians expressed concern over adding in the costs of data repository subscriptions. Each data repository has their own funding and revenue model but many rely on libraries or researchers to pay at some point, whether through subscription costs or pay to publish models.

Many research institutions already have servers to backup and archive data for researchers. But, except for institutional-specific data repositories, few repositories make it possible for institutions to leverage their existing infrastructure while publishing data to a common platform.

This means universities may have to pay twice to backup the same data, once on their institutional servers and once on a common data repository platform. Because of the reliance on existing web infrastructure using location-based addressing, this also means that the data published in two places will be invisible to each other, meaning they could get out of sync or forgotten. If the data platform the data is published is shuts down, external researchers may not be able to access the data even though it is still archived on the university's server.

### Web Infrastructure and Location-Based Addressing

The issue of long term reproducibility and data repository services closing is exacerbated by the fact that many data repositories use HTTP or FTP infrastructure which isolates data with location-based addressing - creating a single point of failure. If the data citation problem is solved then an era of papers containing broken data links is imminent.

Most data repositories have a sustainability and preservation policy, to archive the data in the event the organization shuts down. If the repository shuts down, the data may be migrated to other repositories or backup servers. But we must acknowledge the fact that the web and location-based addressing does not have the same policy. If a repository shuts down then any data published to that repository could be impossible to find. Disappearing websites are a major issue for the web, one that the Internet Archive is trying to mitigate, but the same issue could prove fatal for scientific integrity.

## The Long Tail of Data and the Reproducibility Crisis

Online distribution makes it much easier for users to share and discover long tails. For example, Amazon made the long tail of books available -- only a few people may be interested in each book but there are many many of them. Overall, demand for items in long tail significantly exceeds the demand for the most popular items. There are two parallel ideas in research data.

First, there is a long tail of data that researchers are interested in, similar to rare books on Amazon. These datasets may only have a few downloads but overall they have more demand than popular datasets, such as the Sloan Digital Sky Survey. Secondly, and more importantly, there is a long tail of researchers needing to publish data that may not fit within existing metadata, standards, or data repositories. In order to ensure reproducibility, it is essential to have a place for these researchers to publish and preserve their data.

In the RDA session of the long tail interest group, several presenters made the point that much of the innovative work in research or around data will be happening in this long tail. Fields that are niche, types of data to unique to be standardized, and new uses of existing data, will not fit into old standards. If these types of research and data cannot be shared and archived, then reproducing much of published science will be impossible.

Unless you already know what data repository your data fits into (because its a specific domain, standard, etc.), publishing long tail data can be a challenge. Researchers have to decide on which, if any, general access repository fits their needs and the constraints of that use case. Unfortunately, publishing this type of data will only happen if researchers have a data publishing requirement or strong incentive, which is often missing in the long tail.

If we want to succeed in ensuring data citability and reproducibility, we have to understand how data from the long tail will fit. Otherwise, we will lose the majority of research results to data that does not fit in existing platforms or where the difficulty of using the tools outweighs the benefit researchers get.

## Two Models of Data Publishing

Throughout the week, I started seeing two distinct approaches to sharing scientific data, for lack of better words, a top-down model and a bottom-up model.

In the top-down model the primary goal is to **publish** and cite data. For data to be widely citable it must have a few properties:

* Have metadata that fits with an existing standard (usually the standards are repository-specific)
* Be published in location that is publicly accessible and licensed for re-use
* Be discoverable through a centralized portal

*Note: I use **publish** very explicitly to parallel the current model of paper publishing. Publish does not mean share or make open in this model.*

The primary goal of the bottom-up model is to share data. In the bottom-up model, researchers often rely on existing tools to disseminate data. Until recently, this meant physically sharing USB drives or sending emails to collaborators or researchers trying to reproduce results. Researchers are moving to other tools to increasing availability or ease of sharing such as Dropbox, GitHub, or other general cloud service providers (such as Amazon S3). In this model, the data may not be immediately citable, publicly accessible, or discoverable.
